---
title: "Predicting Users' Barbell Lift Technique"
author: "Charles Bryan"
date: "11/7/2019"
output:
  github_document:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Synopsis

The task here is to determine how barbell lifts were performed based on measurements taken from the participants doing the exercise. The barbell lifts were performed in 5 different ways by different participants where 4 of the ways were incorrect in some common manner. After looking at the provided training data we can see that many columns are left empty or NA, so we split the training data into two separate sets: one set with almost all columns containing data and one set with most columns containing NA or no data. We clean up the data and then perform random forests on the two sets. Both training models perform perfectly on their own training sets, however, they differ on the provided test set. The model trained on the larger training set performed perfectly on the provided test set, while the second model only gets 75% correct. This is likely because the large difference in observations used to train each model: 19216 for model 1 and only 406 for model 2.

## Setup
```{r echo = TRUE, results = 'hide', message=FALSE}
require(tidyverse)
require(caret)
require(gridExtra)
require(naniar)
require(DMwR)
```

```{r Reproducibility}
set.seed(123) 
# sessionInfo()
```

<br>

***
## Data Retrieval
First we import the training and test data provided here:
http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har

Citation:
Ugulino, W.; Cardador, D.; Vega, K.; Velloso, E.; Milidiu, R.; Fuks, H. Wearable Computing: Accelerometers' Data Classification of Body Postures and Movements. Proceedings of 21st Brazilian Symposium on Artificial Intelligence. Advances in Artificial Intelligence - SBIA 2012. In: Lecture Notes in Computer Science. , pp. 52-61. Curitiba, PR: Springer Berlin / Heidelberg, 2012. ISBN 978-3-642-34458-9. DOI: 10.1007/978-3-642-34459-6_6.
```{r}
if(!file.exists("./data")){dir.create("./data")}

# Weight Lifting Exercise Dataset (Separated into training and testing sets)
dataTrainUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
TrainfileName <- "./data/WeightliftTraining.csv"
if(!file.exists(TrainfileName)){download.file(dataTrainUrl, destfile=TrainfileName)}

dataTestUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
TestfileName <- "./data/WeightliftTesting.csv"
if(!file.exists(TestfileName)){download.file(dataTestUrl, destfile=TestfileName)}

training_raw <- read.csv(TrainfileName)
testing_raw <- read.csv(TestfileName)
```

<br>

***

## Data Cleaning and Exploratory Analysis

<br>

The test data provided is not time sequential, as in we cannot use multiple observations together to predict the exercise. Based on this, no time related columns are going to be helpful. Therefore, we remove these columns from both the training and testing sets.
```{r cache = TRUE}
training_sub <- subset(training_raw,select=-c(X, user_name,raw_timestamp_part_1,
                                              raw_timestamp_part_2,
                                              cvtd_timestamp, new_window,
                                              num_window))
testing_sub <- subset(testing_raw, select=-c(X, user_name,raw_timestamp_part_1,
                                             raw_timestamp_part_2, 
                                             cvtd_timestamp, new_window,
                                             num_window))
```

<br>


```{r cache = TRUE, warning = FALSE, error = FALSE}
training_temp <- data.frame(lapply(training_sub, function(x) {gsub("^$", NA, x)}))
vis_miss(training_temp, warn_large_data = FALSE)
```

<br>

This data set has a few different types of errors:  Occurrences of "", NA, and DIV/0.
In total we have 19622 observations. Exactly 19216 of these rows have either "" or NA in many of the cells while the remaining 406 observations have DIV/0 errors in some of their cells. It appears that for these 406 rows additional information was calculated and the method used to calculate some values gave a DIV/0 error in those cases. 
```{r}
na_rows <- apply(training_temp, 1, function(r) any(r %in% c(NA)))

classeCount <- training_temp %>% group_by(classe) %>% summarise(no_rows = length(classe))
classeCountNoNA <- training_temp[!na_rows,] %>% 
  group_by(classe) %>%summarise(no_rows=length(classe))

classeCountNoNA$no_rows/classeCount$no_rows
```
Interestingly each of the 5 classes is impacted by the NA errors relatively similarly. If one class was more significantly impacted we would need to take that into account. 


We will divide the dataset up into two sets, the 406 rows with this additional info (training_sub_div) and the 19216 rows that do not (training_sub_na). This is done in case these additional features prove to be useful in classifying the exercises.
```{r}
training_sub_na <- training_temp[na_rows,]
training_sub_div <- training_temp[!na_rows,]
```

<br>

Now we clean up the data some:

* We filter out columns that only have 1 value since this information is not helpful in classification. 
* We convert all columns other than the classe label into numeric variables.
* In the training_sub_div dataset we convert the "DIV/0!" error into NA to more easily work with.
```{r}
# Update Factor Levels based on the current dataframe, not the original
training_sub_na[] <- lapply(training_sub_na, function(x) if(is.factor(x)) factor(x) else x)
# Remove Columns with only 1 Factor level. These columns will not contribute to a model.
training_sub_na <- training_sub_na[, sapply(training_sub_na, function(col) length(unique(col))) > 1]
# Convert all columns to type numeric except the classe label
names_na <- colnames(training_sub_na)
names_na <- names_na[!names_na %in% c("classe")]
training_sub_na[names_na] <- lapply(training_sub_na[names_na], 
                                           function(x) as.numeric(levels(x))[x])

# Replace DIV/0 error with NA
training_sub_div <- data.frame(lapply(training_sub_div, function(x) {gsub("#DIV/0!", NA, x)}))
# Update Factor Levels based on the current dataframe, not the original
training_sub_div[] <- lapply(training_sub_div, function(x) if(is.factor(x)) factor(x) else x)
# Remove Columns with only 1 Factor level. These columns will not contribute to a model.
training_sub_div <- training_sub_div[, sapply(training_sub_div, function(col){
  length(levels(col))}) > 1]
```

<br>

We can see below that we are still missing some values in the training_sub_div dataset.
```{r cache = TRUE}
vis_miss(training_sub_div, warn_large_data = FALSE)
```

<br>

These rows with missing data make up a significant portion of the 406 observations in this set, so we would prefer to not remove these rows. These missing values are numeric, so we try to impute the values using default knn.
```{r}
training_sub_div_clean <- knnImputation(training_sub_div, k = 10, scale = T, 
                                        meth = "weighAvg", distData = training_sub_div)
```

<br>

As we did before we will clean up this dataset of any columns that have only one value and also convert all features to numeric other than the classe labels.
```{r}
# Convert all but the classe label from Factor to numeric
names_div <- colnames(training_sub_div_clean)
names_div <- names_div[!names_div %in% c("classe")]
training_sub_div_clean[names_div] <- lapply(training_sub_div_clean[names_div], 
                                            function(x) as.numeric(levels(x))[x])

# Remove Columns with only 1 Numeric value. This ends up only being the column
# "amplitude_yaw_dumbbell" with only the value 0
training_sub_div_clean <- training_sub_div_clean[, sapply(training_sub_div_clean, function(col){length(unique(col))}) > 1]
```

<br>

I used the following method to plot histograms of each variable to visually find any outliers. Unfortunately due to the amount of features the plot does not fit well in one viewing.
```{r, echo=TRUE, include=FALSE, cache = TRUE}
 training_sub_div_clean %>%
  keep(is.numeric) %>%
  gather() %>%
  ggplot(aes(value)) +
    facet_wrap(~ key, scales = "free") +
    geom_histogram()
```

<br>

Rather than showing all of the feature plots, below is one such histogram. Because the x-axis goes so far to the right with all of the data focused on the far left, we know to inspect the data. It turns out for this feature almost all of the data is beneath the value 60 with a couple of extremely high outliers in the tens of thousands.
```{r}
ggplot(data=training_sub_div_clean, aes(x=var_yaw_belt)) + geom_histogram()
```

<br>

Based on the plots and further inspecting the data, we filter out some outliers of each dataset using the following criteria:
```{r}
training_na_filter <- training_sub_na %>%
  filter(magnet_belt_x < 350) %>%
  filter(magnet_belt_z < 100) %>%
  filter(gyros_forearm_z > -6) %>%
  filter(gyros_dumbbell_x > -100) %>%
  filter(magnet_dumbbell_y > -2000)
```

```{r}
training_div_filter <- training_sub_div_clean %>%
  filter(amplitude_roll_belt < 300) %>%
  filter(kurtosis_picth_arm < 15) %>%
  filter(kurtosis_picth_dumbbell < 40) %>%
  filter(kurtosis_roll_arm < 15) %>%
  filter(kurtosis_roll_belt < 30) %>%
  filter(kurtosis_roll_dumbbell < 40) %>%
  filter(kurtosis_roll_forearm < 30) %>%
  filter(magnet_belt_x < 350) %>%
  filter(magnet_belt_z < 100) %>%
  filter(max_yaw_belt < 30) %>%
  filter(max_yaw_dumbbell < 40) %>%
  filter(max_yaw_forearm < 40) %>%
  filter(min_yaw_dumbbell < 40) %>%
  filter(min_yaw_forearm < 30) %>%
  filter(stddev_roll_arm < 150) %>%
  filter(stddev_yaw_belt < 80) %>%
  filter(var_accel_dumbbell < 100) %>%
  filter(var_pitch_dumbbell < 6000) %>%
  filter(var_roll_arm < 20000) %>%
  filter(var_yaw_arm < 15000) %>%
  filter(var_yaw_belt < 60) %>%
  filter(var_yaw_dumbbell < 7000) %>%
  filter(gyros_forearm_z > -6) %>%
  filter(gyros_dumbbell_x > -100) %>%
  filter(magnet_dumbbell_y > -2000)
```

<br>

***
## Modeling

<br>

While it was not taken into account during the data cleaning, the testing set has many rows that are entirely NA. This makes them unusable for classification. First we determine which columns are usable in the testing set.
```{r}
testing_columns <- colnames(testing_sub[1,!sapply(testing_sub, function(x)all(is.na(x)))])
length(testing_columns)
```
We see that only 53 of the original columns are actually usable.

<br>

We will build two models, both of which using random forests and 10-fold cross validation.

modFit1 will be using our large dataset of 19216 observations which removed all of the features that contained NAs. We only train the model with the columns that this set and the testing set share.
```{r Train Model1 on Large Dataset, cache=TRUE}
training_na_columns <- colnames(training_na_filter)
training_na_intersection <- intersect(testing_columns, training_na_columns)

#Setup our cross validation. We will use this in model2 as well
train_control<- trainControl(method="cv", number=10)

modFit1 <- train(classe~., 
                 data=training_na_filter[,c(training_na_intersection, "classe")],
                 trControl=train_control,
                 method="rf")

training_predict1 <- predict(modFit1)
sum(training_predict1 == training_na_filter[,"classe"])/length(training_predict1)
```
This model had a perfect accuracy on its training set of 19216 observations with a very low OOB estimate of  error rate of 0.44%.

<br>

```{r modFit1 confusionMatrix}
modFit1$finalModel$confusion
```

<br>

Similarly we build modFit2 using the dataset of 406 observations and 10-fold cross validation. This dataset has the additional columns that were not present for the 19216 observations above. However, we can only train the model on features shared between this dataset and the testing set.
```{r Train Model2 on Smaller Dataset, cache=TRUE}
training_div_columns <- colnames(training_div_filter)
training_div_intersection <- intersect(testing_columns, training_div_columns)

modFit2 <- train(classe~., 
                 data=training_div_filter[,c(training_div_intersection, "classe")],
                 trControl=train_control,
                 method="rf")

training_predict2 <- predict(modFit2)
sum(training_predict2 == training_div_filter[,"classe"])/length(training_predict2)
```
This model had a perfect accuracy on its training set of 406 observations, however, we can see that we have a high OOB estimate of  error rate: 21.04%.

<br>

```{r modFit2 confusionMatrix}
modFit2$finalModel$confusion
```


<br>

Now we run each of our models on the testing set.
```{r Predict testing data using both models}
testing_predict1 <- predict(modFit1, newdata = testing_sub[,training_na_intersection])
testing_predict2 <- predict(modFit2, newdata = testing_sub[,training_div_intersection])
```

<br>

We can check to see if our two models predicted different results.
```{r}
sum(testing_predict1==testing_predict2)/length(testing_predict1)
```
It appears that the predictions were the same for only 75% of the testing observations. For the 20 quiz questions I used the results from the larger set as it was more reliable based on the model error estimates and the sheer number of observations used for training. This model received a perfect score on the limited test set provided.

<br>

***

## Conclusions

1. I would expect little error when using modFit1 based on the very low error estimate, however, I would expect a large error rate using the second model.
2. Looking back at the steps I took for data cleaning, I should not have split the data based into a group of 19216 and 406 observations. This was done thinking the extra features would be helpful in the dataset of 406, however, the test set did not contian the additional features provided by this set making them no more helpful.
